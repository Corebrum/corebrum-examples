task_definition:
  name: "person_follow_simple"
  version: "1.0"
  description: "Simple person following using Qwen2.5VL for human detection"
  
  execution_mode: "stream_reactive"
  stream_config:
    trigger: "on_message"
    rate_limit_hz: 2  # Process every 2nd image
    buffer_size: 1
  
  inputs:
    - name: "camera_image"
      type: "zenoh_topic"
      key_expr: "rt/camera/camera/color/image_raw"
      encoding: "raw"
  
  outputs:
    - name: "velocity_command"
      type: "zenoh_topic"
      key_expr: "rt/cmd_vel"
      encoding: "raw"
  
  compute_logic:
    type: "script"
    language: "python"
    timeout_seconds: 20
    code: |
      import struct
      import base64
      import cv2
      import numpy as np
      import requests
      import json
      import time
      import logging
      
      # Configure logging
      logging.basicConfig(level=logging.INFO)
      logger = logging.getLogger(__name__)
      
      def parse_ros2_image(payload):
          """Parse ROS2 sensor_msgs/Image message format manually (from working code)"""
          try:
              # Convert ZBytes to bytes if needed
              if hasattr(payload, 'data'):
                  payload_bytes = payload.data
              else:
                  payload_bytes = bytes(payload)
              
              # Find the frame_id by looking for "camera" string
              camera_pos = payload_bytes.find(b'camera')
              if camera_pos == -1:
                  logger.error("Could not find 'camera' frame_id in payload")
                  return None
              
              # The frame_id length should be 4 bytes before "camera"
              frame_id_len_pos = camera_pos - 4
              if frame_id_len_pos < 0:
                  logger.error("Invalid frame_id length position")
                  return None
              
              frame_id_len = struct.unpack('<I', payload_bytes[frame_id_len_pos:frame_id_len_pos+4])[0]
              
              # Calculate the offset after frame_id
              frame_id_end = camera_pos + frame_id_len
              
              # Parse image dimensions
              height = struct.unpack('<I', payload_bytes[frame_id_end:frame_id_end+4])[0]
              width = struct.unpack('<I', payload_bytes[frame_id_end+4:frame_id_end+8])[0]
              
              # Find the "rgb8" encoding string
              rgb8_pos = payload_bytes.find(b'rgb8')
              if rgb8_pos == -1:
                  logger.error("Could not find 'rgb8' encoding in payload")
                  return None
              
              # Calculate image data size and find matching resolution
              total_payload = len(payload_bytes)
              header_size = rgb8_pos + 5 + 1 + 4  # encoding + is_bigendian + step
              image_data_size = total_payload - header_size
              
              # Try common camera resolutions
              common_resolutions = [
                  (640, 480), (800, 600), (1024, 768), (1280, 720), (1280, 960), 
                  (1920, 1080), (1280, 1024), (1600, 1200)
              ]
              
              for w, h in common_resolutions:
                  expected_size = w * h * 3  # 3 channels for RGB
                  if abs(expected_size - image_data_size) < 100:  # Allow small tolerance for padding
                      width, height = w, h
                      break
              else:
                  # Fallback to calculated dimensions
                  pixels = image_data_size // 3
                  width = int(pixels ** 0.5)
                  height = pixels // width
              
              # Get image data
              offset = rgb8_pos + 5 + 1 + 4  # encoding + is_bigendian + step
              image_data = payload_bytes[offset:]
              
              # Convert ROS2 Image to OpenCV format
              image_array = np.frombuffer(image_data, dtype=np.uint8)
              
              # Truncate to expected size if there's extra data
              expected_pixels = height * width * 3
              if len(image_array) > expected_pixels:
                  image_array = image_array[:expected_pixels]
              
              image_array = image_array.reshape((height, width, 3))
              
              # Convert RGB to BGR for OpenCV
              image_bgr = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
              
              logger.info(f"Successfully parsed ROS2 image: {image_bgr.shape}")
              return image_bgr
                  
          except Exception as e:
              logger.error(f"Error parsing ROS2 image: {e}")
              return None
      
      def detect_human_qwen(image):
          """Use Qwen to detect human"""
          try:
              # Resize for faster processing
              resized = cv2.resize(image, (160, 90))
              _, buffer = cv2.imencode('.jpg', resized, [cv2.IMWRITE_JPEG_QUALITY, 85])
              image_b64 = base64.b64encode(buffer).decode('utf-8')
              
              # Ask Qwen if there's a human
              payload = {
                  "model": "qwen2.5vl:3b",
                  "prompt": "Is there a human person in this image? Answer only 'yes' or 'no'.",
                  "images": [image_b64],
                  "stream": False
              }
              
              response = requests.post(
                  "http://localhost:11434/api/generate",
                  json=payload,
                  timeout=10
              )
              
              if response.status_code == 200:
                  result = response.json()
                  answer = result.get('response', 'no').strip().lower()
                  return 'yes' in answer or 'human' in answer
              else:
                  logger.error(f"Qwen API error: {response.status_code}")
                  return False
                  
          except Exception as e:
              logger.error(f"Error detecting human: {e}")
              return False
      
      def create_twist_binary(linear_x, angular_z):
          """Create ROS2 Twist message in binary format"""
          buffer = bytearray(48)  # 6 doubles * 8 bytes
          struct.pack_into('<ddd', buffer, 0, linear_x, 0.0, 0.0)  # linear
          struct.pack_into('<ddd', buffer, 24, 0.0, 0.0, angular_z)  # angular
          return bytes(buffer)
      
      # Main processing
      logger.info("Processing camera image for person following...")
      
      # Parse the image
      image = parse_ros2_image(camera_image)
      
      if image is not None:
          # Log image details
          logger.info(f"ðŸ“¸ Image received: {image.shape[1]}x{image.shape[0]} pixels")
          
          # Save a debug image (optional - for debugging)
          try:
              debug_filename = f"/tmp/debug_image_{int(time.time())}.jpg"
              cv2.imwrite(debug_filename, image)
              logger.info(f"ðŸ’¾ Debug image saved to: {debug_filename}")
          except Exception as e:
              logger.warning(f"Could not save debug image: {e}")
          
          # Detect human
          human_detected = detect_human_qwen(image)
          
          if human_detected:
              logger.info("ðŸ¤– Human detected! Moving forward...")
              linear_x = 0.3  # Move forward
              angular_z = 0.0  # No turning
          else:
              logger.info("ðŸ¤– No human detected. Stopping...")
              linear_x = 0.0  # Stop
              angular_z = 0.0  # No turning
      else:
          logger.warning("Failed to parse image, stopping robot")
          linear_x = 0.0
          angular_z = 0.0
      
      # Create velocity command
      twist_data = create_twist_binary(linear_x, angular_z)
      
      # Log velocity command details
      logger.info(f"ðŸš— Velocity command: linear_x={linear_x}, angular_z={angular_z}")
      logger.info(f"ðŸ“¤ Publishing to zenoh topic: rt/cmd_vel")
      
      # Return result
      result = {
          "velocity_command": twist_data
      }
  
  validation: []
  metadata:
    robot: "person_follower"
    behavior: "follow_person_qwen"
    model: "qwen2.5vl:3b"
    control_type: "binary_forward_stop"
    description: "Simple person following using Qwen2.5VL vision model"
