task_definition:
  name: "person_follow_qwen"
  version: "1.0"
  description: "Follow person using Qwen2.5VL vision model for human detection and ROS2 control"
  
  execution_mode: "stream_reactive"
  stream_config:
    trigger: "on_message"
    rate_limit_hz: 5  # Process every 5th image to avoid overwhelming the model
    buffer_size: 1
  
  inputs:
    - name: "camera_image"
      type: "zenoh_topic"
      key_expr: "rt/camera/camera/color/image_raw"
      encoding: "raw"
  
  outputs:
    - name: "velocity_command"
      type: "zenoh_topic"
      key_expr: "rt/cmd_vel"
      encoding: "raw"
    - name: "analysis_result"
      type: "zenoh_topic"
      key_expr: "image/qwen/person_detection"
      encoding: "raw"
  
  compute_logic:
    type: "script"
    language: "python"
    timeout_seconds: 30
    code: |
      import asyncio
      import logging
      import struct
      import base64
      import cv2
      import numpy as np
      import requests
      import json
      import time
      from typing import Optional
      
      # Configure logging
      logging.basicConfig(level=logging.INFO)
      logger = logging.getLogger(__name__)
      
      class PersonFollowController:
          def __init__(self, ollama_url: str = "http://localhost:11434"):
              self.ollama_url = ollama_url
              self.image_count = 0
              
          def parse_ros2_image(self, payload) -> Optional[np.ndarray]:
              """Parse ROS2 sensor_msgs/Image message format manually (from working code)"""
              try:
                  # Convert ZBytes to bytes if needed
                  if hasattr(payload, 'data'):
                      payload_bytes = payload.data
                  else:
                      payload_bytes = bytes(payload)
                  
                  # Find the frame_id by looking for "camera" string
                  camera_pos = payload_bytes.find(b'camera')
                  if camera_pos == -1:
                      logger.error("Could not find 'camera' frame_id in payload")
                      return None
                  
                  # The frame_id length should be 4 bytes before "camera"
                  frame_id_len_pos = camera_pos - 4
                  if frame_id_len_pos < 0:
                      logger.error("Invalid frame_id length position")
                      return None
                  
                  frame_id_len = struct.unpack('<I', payload_bytes[frame_id_len_pos:frame_id_len_pos+4])[0]
                  
                  # Calculate the offset after frame_id
                  frame_id_end = camera_pos + frame_id_len
                  
                  # Parse image dimensions
                  height = struct.unpack('<I', payload_bytes[frame_id_end:frame_id_end+4])[0]
                  width = struct.unpack('<I', payload_bytes[frame_id_end+4:frame_id_end+8])[0]
                  
                  # Find the "rgb8" encoding string
                  rgb8_pos = payload_bytes.find(b'rgb8')
                  if rgb8_pos == -1:
                      logger.error("Could not find 'rgb8' encoding in payload")
                      return None
                  
                  # Calculate image data size and find matching resolution
                  total_payload = len(payload_bytes)
                  header_size = rgb8_pos + 5 + 1 + 4  # encoding + is_bigendian + step
                  image_data_size = total_payload - header_size
                  
                  # Try common camera resolutions
                  common_resolutions = [
                      (640, 480), (800, 600), (1024, 768), (1280, 720), (1280, 960), 
                      (1920, 1080), (1280, 1024), (1600, 1200)
                  ]
                  
                  for w, h in common_resolutions:
                      expected_size = w * h * 3  # 3 channels for RGB
                      if abs(expected_size - image_data_size) < 100:  # Allow small tolerance for padding
                          width, height = w, h
                          break
                  else:
                      # Fallback to calculated dimensions
                      pixels = image_data_size // 3
                      width = int(pixels ** 0.5)
                      height = pixels // width
                  
                  # Get image data
                  offset = rgb8_pos + 5 + 1 + 4  # encoding + is_bigendian + step
                  image_data = payload_bytes[offset:]
                  
                  # Convert ROS2 Image to OpenCV format
                  image_array = np.frombuffer(image_data, dtype=np.uint8)
                  
                  # Truncate to expected size if there's extra data
                  expected_pixels = height * width * 3
                  if len(image_array) > expected_pixels:
                      image_array = image_array[:expected_pixels]
                  
                  image_array = image_array.reshape((height, width, 3))
                  
                  # Convert RGB to BGR for OpenCV
                  image_bgr = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
                  
                  logger.info(f"Successfully parsed ROS2 image: {image_bgr.shape}")
                  return image_bgr
                      
              except Exception as e:
                  logger.error(f"Error parsing ROS2 image: {e}")
                  return None
          
          def detect_human_with_qwen(self, image: np.ndarray) -> bool:
              """Use Qwen2.5VL to detect if there's a human in the image"""
              try:
                  # Resize image to small size for faster processing
                  resized_image = cv2.resize(image, (160, 90))
                  
                  # Convert image to base64 with compression
                  _, buffer = cv2.imencode('.jpg', resized_image, [cv2.IMWRITE_JPEG_QUALITY, 85])
                  image_base64 = base64.b64encode(buffer).decode('utf-8')
                  
                  # Prepare the request payload for human detection
                  payload = {
                      "model": "qwen2.5vl:3b",  # Fast 3.8B parameter Qwen vision model
                      "prompt": "Is there a human person visible in this image? Answer only 'yes' or 'no'.",
                      "images": [image_base64],
                      "stream": False
                  }
                  
                  logger.info("Sending image to Qwen for human detection...")
                  
                  # Send request to Ollama
                  response = requests.post(
                      f"{self.ollama_url}/api/generate",
                      json=payload,
                      timeout=15  # Timeout for 160x90 images with yes/no responses
                  )
                  
                  if response.status_code == 200:
                      result = response.json()
                      answer = result.get('response', 'no').strip().lower()
                      human_detected = 'yes' in answer or 'human' in answer or 'person' in answer
                      logger.info(f"ðŸŽ¯ Qwen Human Detection: {answer} -> {human_detected}")
                      return human_detected
                  else:
                      logger.error(f"Ollama API error: {response.status_code} - {response.text}")
                      return False
                      
              except Exception as e:
                  logger.error(f"Error detecting human with Qwen: {e}")
                  return False
          
          def create_twist_message(self, linear_x: float, angular_z: float) -> bytes:
              """Create ROS2 geometry_msgs/Twist message in binary format"""
              try:
                  # Create a buffer for the Twist message
                  # geometry_msgs/Twist has 6 doubles: linear.x, linear.y, linear.z, angular.x, angular.y, angular.z
                  buffer = bytearray(48)  # 6 * 8 bytes = 48 bytes
                  
                  # Pack linear velocities (x, y, z)
                  struct.pack_into('<ddd', buffer, 0, linear_x, 0.0, 0.0)
                  
                  # Pack angular velocities (x, y, z)
                  struct.pack_into('<ddd', buffer, 24, 0.0, 0.0, angular_z)
                  
                  return bytes(buffer)
                  
              except Exception as e:
                  logger.error(f"Error creating Twist message: {e}")
                  # Return stop command on error
                  return self.create_twist_message(0.0, 0.0)
      
      # Initialize the controller
      controller = PersonFollowController()
      
      # Process the camera image
      logger.info(f"Processing camera image #{controller.image_count + 1}")
      controller.image_count += 1
      
      # Only process every 5th image to avoid overwhelming the model
      if controller.image_count % 5 != 0:
          logger.info(f"Skipping image #{controller.image_count} (rate limiting)")
          # Still publish stop command
          twist_data = controller.create_twist_message(0.0, 0.0)
          analysis_result = json.dumps({
              "human_detected": False,
              "reason": "rate_limiting",
              "timestamp": time.time()
          })
      else:
          # Parse the ROS2 image
          image = controller.parse_ros2_image(camera_image)
          
          if image is not None:
              # Log image details
              logger.info(f"ðŸ“¸ Image received: {image.shape[1]}x{image.shape[0]} pixels")
              
              # Save a debug image (optional - for debugging)
              try:
                  debug_filename = f"/tmp/debug_image_qwen_{int(time.time())}.jpg"
                  cv2.imwrite(debug_filename, image)
                  logger.info(f"ðŸ’¾ Debug image saved to: {debug_filename}")
              except Exception as e:
                  logger.warning(f"Could not save debug image: {e}")
              
              # Detect human using Qwen
              human_detected = controller.detect_human_with_qwen(image)
              
              # Create velocity command based on detection
              if human_detected:
                  linear_x = 0.3  # Move forward at 0.3 m/s
                  angular_z = 0.0  # No turning for now
                  logger.info("ðŸ¤– Human detected! Moving forward...")
              else:
                  linear_x = 0.0  # Stop
                  angular_z = 0.0  # No turning
                  logger.info("ðŸ¤– No human detected. Stopping...")
              
              # Create Twist message
              twist_data = controller.create_twist_message(linear_x, angular_z)
              
              # Log velocity command details
              logger.info(f"ðŸš— Velocity command: linear_x={linear_x}, angular_z={angular_z}")
              logger.info(f"ðŸ“¤ Publishing to zenoh topic: rt/cmd_vel")
              
              # Create analysis result
              analysis_result = json.dumps({
                  "human_detected": human_detected,
                  "linear_x": linear_x,
                  "angular_z": angular_z,
                  "timestamp": time.time()
              })
          else:
              # Failed to parse image, stop robot
              logger.warning("Failed to parse image, stopping robot")
              twist_data = controller.create_twist_message(0.0, 0.0)
              analysis_result = json.dumps({
                  "human_detected": False,
                  "reason": "parse_error",
                  "timestamp": time.time()
              })
      
      # Return the results
      result = {
          "velocity_command": twist_data,
          "analysis_result": analysis_result.encode('utf-8')
      }
  
  validation: []
  metadata:
    robot: "person_follower"
    behavior: "follow_person_qwen"
    model: "qwen2.5vl:3b"
    control_type: "binary_forward_stop"
    description: "Uses Qwen2.5VL vision model to detect humans and control robot movement"
